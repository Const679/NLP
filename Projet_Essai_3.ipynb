{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qqVMknts1K-",
        "outputId": "96f2195a-2001-4631-a354-f9e312c606c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Cloning into 'project1-2023'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (8/8), 2.30 MiB | 2.90 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!git clone https://github.com/cr-nlp/project1-2023.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lvjXLXGs1LD"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nZjMOKf1s1LF"
      },
      "outputs": [],
      "source": [
        "import urllib.request as re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz1ZrT22s1LG",
        "outputId": "af1c9fd4-6cb1-4a48-8c3d-1c2e5de4c537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOCGoh1ts1LH"
      },
      "source": [
        "## Import des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0_cwEqADs1LH"
      },
      "outputs": [],
      "source": [
        "#Code prof Import des 3 fichiers\n",
        "\n",
        "def loadNFCorpus():\n",
        "\tdir = \"./project1-2023/\"\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\n",
        "\tdicDoc={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\t#print(tabLine)\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\t#print(value)\n",
        "\t\tdicDoc[key] = value\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5pTg1P23s1LI"
      },
      "outputs": [],
      "source": [
        "dicDoc, dicReq, dicReqDoc = loadNFCorpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqv1mMjJs1LJ"
      },
      "source": [
        "## Mise en forme de la donnée, enlèvement des mots inutiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v3TonkIMs1LJ"
      },
      "outputs": [],
      "source": [
        "# Mise en forme des article, enlèvement des mots inutiles\n",
        "def text2TokenList(text):\n",
        "\tstopword = stopwords.words('english')\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\n",
        "\treturn word_tokens_without_stops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7ba94hhQs1LJ"
      },
      "outputs": [],
      "source": [
        "docsToKeep=[]\n",
        "reqsToKeep=[]\n",
        "dicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "#Construction du dictionnaire de Req à garder\n",
        "i=0\n",
        "for reqId in dicReqDoc:\n",
        "    if i >= len(dicDoc) :  #nbDocsToKeep:\n",
        "        break\n",
        "    for docId in dicReqDoc[reqId]:\n",
        "        dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "        docsToKeep.append(docId)\n",
        "        i = i + 1\n",
        "    reqsToKeep.append(reqId)\n",
        "docsToKeep = list(set(docsToKeep))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0BEheJAs1LK"
      },
      "source": [
        "## Construction des corpus des MED et PLAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a54DOuJZs1LK"
      },
      "outputs": [],
      "source": [
        "allVocab ={}\n",
        "#Construction corpus des MED\n",
        "for k in docsToKeep: #Pour chaque MED\n",
        "    docTokenList = text2TokenList(dicDoc[k]) #Transforme les texte de MED en liste\n",
        "    #print(docTokenList)\n",
        "    for word in docTokenList: #Pour chaque mot dans la liste\n",
        "        if word not in allVocab: #S'il n'y est pas déjà\n",
        "            allVocab[word] = word #l'ajoute dans le dictionnaire\n",
        "\n",
        "allVocabListDoc = list(allVocab)\n",
        "#print(\"doc vocab=\",allVocabListDoc)\n",
        "\n",
        "\n",
        "allVocab ={}\n",
        "\n",
        "#Construction corpus des PLAIN\n",
        "for k in reqsToKeep: #Pour chaque PLAIN\n",
        "    docTokenList = text2TokenList(dicReq[k]) # Transforme les texte de PLAIN en liste\n",
        "    #print(docTokenList)\n",
        "    for word in docTokenList: # Pour chaque mot dans la liste\n",
        "        if word not in allVocab: #S'il n'y est pas déjà\n",
        "            allVocab[word] = word #l'ajoute dans le dictionnaire\n",
        "allVocabListReq = list(allVocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tLCLJkiqs1LK"
      },
      "outputs": [],
      "source": [
        "corpusDocTokenList = {}\n",
        "corpusReqTokenList = {}\n",
        "corpusDocName=[]\n",
        "corpusDicoDocName={}\n",
        "i = 0\n",
        "\n",
        "for k in docsToKeep: #Pour chaque MED\n",
        "    docTokenList = text2TokenList(dicDoc[k]) #Transforme son corpus en liste\n",
        "    corpusDocTokenList[k]=docTokenList #complete le dictionnaire des mots pour chaque MED\n",
        "    corpusDocName.append(k) #Ajoute le nom du MED dasn sa propre liste\n",
        "    corpusDicoDocName[k] = i # Attribue un numero a chaque MED\n",
        "    i = i + 1\n",
        "\n",
        "#print(\"reqs...\")\n",
        "corpusReqName=[]\n",
        "corpusDicoReqName={}\n",
        "i = 0\n",
        "for k in reqsToKeep: # Pour tous les PLAIN\n",
        "    reqTokenList = text2TokenList(dicReq[k]) #Transforme son corpus en liste\n",
        "    corpusReqTokenList[k] = reqTokenList#complete le dictionnaire des mots pour chaque PLAIN\n",
        "    corpusReqName.append(k) #Ajoute le nom du PLAIN dans sa propre liste\n",
        "    corpusDicoReqName[k] = i# Attribue un numero a chaque PLAIN\n",
        "    i = i + 1"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "dG9noNN_s1LL"
      },
      "source": [
        "Arrivé ici, on a :\n",
        "    - docsToKeep : une liste de tous les nom de MED\n",
        "    - allVocabListDoc : une liste de tous les mots dans les MED sans distinction sans doublons\n",
        "    - allVocabListReq : une liste de tous les mots dans les PLAIN sans distinction sans doublons\n",
        "    - corpusDocTokenList : une liste de tous les mots dans les MED sans distinction avec doublons\n",
        "    - corpusDocName : same as docsToKeep ?\n",
        "    - corpusDicoDocName : numerote chaque MED\n",
        "    - corpusReqTokenList : une liste de tous les mots dans les PLAIN sans distinction avec doublons\n",
        "    - corpusReqName : une liste de tous les nom de PLAIN\n",
        "    - corpusDicoReqName : numerote chaque PLAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSqQRTkCs1LL"
      },
      "source": [
        "## Recherche du score TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO5CimNZs1LL",
        "outputId": "99eb730b-5205-42d5-c379-e62f931296c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03777821460241738\n",
            "CPU times: user 12 s, sys: 84.5 ms, total: 12.1 s\n",
            "Wall time: 16.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "TableTFIDF=pd.DataFrame(columns=['MED','PLAIN','score'])\n",
        "c=0\n",
        "CumulTFIDF=0\n",
        "for plain in corpusReqTokenList: #Pour chaque mot dans le plain avec doublons\n",
        "    j=0\n",
        "    reqTokenList = corpusReqTokenList[plain]  #liste de mots de PLAIN n°req\n",
        "    #print(req)\n",
        "    #print(len(doc_scores)) #nombre de mot dans chaque PLAIN ?\n",
        "    trueDocs = np.zeros(len(corpusDocTokenList)) #Ressort un array de la forme de corpusDocTokenList (1779,)\n",
        "    for med in corpusDicoDocName: #pour chaque nom de MED\n",
        "        if med in dicReqDocToKeep[plain]: # et si MED n°docId est dans les recommendation de PLAIN\n",
        "            vectorizer=TfidfVectorizer() # Déclare un nouveau vecteur\n",
        "            pos=corpusDicoDocName[med] # Recupère la position du MED\n",
        "            textMED= ' '.join(corpusDocTokenList[med]) # Recrée un cropus str à partir de la liste de MED\n",
        "            textPLAIN=' '.join(corpusReqTokenList[plain]) # Recrée un cropus str à partir de la liste de PLAIN\n",
        "            score=vectorizer.fit_transform([textMED,textPLAIN]) # Cherche le score TF IDF des MED et PLAIN associés\n",
        "            trueDocs[corpusDicoDocName[med]] = score.mean() # Somme des moyennes de TFIDF de MED pour PLAIN n°plain\n",
        "    CumulTFIDF=CumulTFIDF + score.mean()\n",
        "    c = c + 1\n",
        "moytfidf=CumulTFIDF/c\n",
        "print(moytfidf)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B9OZQeigs1LL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
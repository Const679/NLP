{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdkYTn2wslYt",
        "outputId": "1707e6e4-1244-4646-e404-7485c4427743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Cloning into 'project1-2023'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (8/8), 2.30 MiB | 2.81 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!git clone https://github.com/cr-nlp/project1-2023.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aBkkKYOKslYx"
      },
      "outputs": [],
      "source": [
        "import urllib.request as re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLxHHSu_slYy",
        "outputId": "6e171b5b-89cb-41f4-fb77-8d5f62588dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mr5eTSl6slYz"
      },
      "outputs": [],
      "source": [
        "def loadNFCorpus():\n",
        "\tdir = \"./project1-2023/\"\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\n",
        "\tdicDoc={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\t#print(tabLine)\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\t#print(value)\n",
        "\t\tdicDoc[key] = value\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mvUNz2ZVslYz"
      },
      "outputs": [],
      "source": [
        "def text2TokenList(text):\n",
        "\tstopword = stopwords.words('english')\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\treturn word_tokens_without_stops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-Q5LIjtgslY0"
      },
      "outputs": [],
      "source": [
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "    dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "    #print(dicReqDoc)\n",
        "\n",
        "    docsToKeep=[]\n",
        "    reqsToKeep=[]\n",
        "    dicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "    #150\n",
        "    ndcgTop=5\n",
        "    print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",endDoc - startDoc)\n",
        "\n",
        "\n",
        "    #Construction du dictionnaire de Req à garder\n",
        "    i=startDoc\n",
        "    for reqId in dicReqDoc:\n",
        "        if i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "            break\n",
        "        for docId in dicReqDoc[reqId]:\n",
        "            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "            docsToKeep.append(docId)\n",
        "            i = i + 1\n",
        "        reqsToKeep.append(reqId)\n",
        "    docsToKeep = list(set(docsToKeep))\n",
        "\n",
        "    #print(docsToKeep)\n",
        "\n",
        "    #\"\"\"\n",
        "    allVocab ={}\n",
        "    #Construction corpus des MED\n",
        "    for k in docsToKeep: #Pour chaque MED\n",
        "        docTokenList = text2TokenList(dicDoc[k]) #Transforme les texte de MED en liste\n",
        "        #print(docTokenList)\n",
        "        for word in docTokenList: #Pour chaque mot dans la liste\n",
        "            if word not in allVocab: #S'il n'y est pas déjà\n",
        "                allVocab[word] = word #l'ajoute dans le dictionnaire\n",
        "\n",
        "    allVocabListDoc = list(allVocab)\n",
        "    #print(\"doc vocab=\",allVocabListDoc)\n",
        "\n",
        "\n",
        "    allVocab ={}\n",
        "\n",
        "    #Construction corpus des PLAIN\n",
        "    for k in reqsToKeep: #Pour chaque PLAIN\n",
        "        docTokenList = text2TokenList(dicReq[k]) # Transforme les texte de PLAIN en liste\n",
        "        #print(docTokenList)\n",
        "        for word in docTokenList: # Pour chaque mot dans la liste\n",
        "            if word not in allVocab: #S'il n'y est pas déjà\n",
        "                allVocab[word] = word #l'ajoute dans le dictionnaire\n",
        "    allVocabListReq = list(allVocab)\n",
        "\n",
        "    from rank_bm25 import BM25Okapi\n",
        "\n",
        "    corpusDocTokenList = []\n",
        "    corpusReqTokenList = {}\n",
        "    corpusDocName=[]\n",
        "    corpusDicoDocName={}\n",
        "    i = 0\n",
        "\n",
        "    for k in docsToKeep: #Pour chaque MED\n",
        "        docTokenList = text2TokenList(dicDoc[k]) #Transforme son corpus en liste\n",
        "        corpusDocTokenList.append(docTokenList) #L'ajoute dans la liste de tous les corpus\n",
        "        corpusDocName.append(k) #Ajoute le nom du MED dasn sa propre liste\n",
        "        corpusDicoDocName[k] = i # Attribue un numero a chaque MED\n",
        "        i = i + 1\n",
        "\n",
        "    #print(\"reqs...\")\n",
        "    corpusReqName=[]\n",
        "    corpusDicoReqName={}\n",
        "    i = 0\n",
        "    for k in reqsToKeep: # Pour tous les PLAIN\n",
        "        reqTokenList = text2TokenList(dicReq[k]) #Transforme son corpus en liste\n",
        "        corpusReqTokenList[k] = reqTokenList#créer un dictionnaire des mots pour chaque PLAIN\n",
        "        corpusReqName.append(k) #Ajoute le nom du PLAIN dans sa propre liste\n",
        "        corpusDicoReqName[k] = i# Attribue un numero a chaque PLAIN\n",
        "        i = i + 1\n",
        "\n",
        "    # Arrivé ici, on a :\n",
        "    #- docsToKeep : une liste de tous les nom de MED\n",
        "    #- allVocabListDoc : une liste de tous les mots dans les MED sans distinction sans doublons\n",
        "    #- allVocabListReq : une liste de tous les mots dans les PLAIN sans distinction sans doublons\n",
        "    #- corpusDocTokenList : une liste de tous les mots dans les MED sans distinction avec doublons\n",
        "    #- corpusDocName : same as docsToKeep ?\n",
        "    #- corpusDicoDocName : numerote chaque MED\n",
        "    #- corpusReqTokenList : une liste de tous les mots dans les PLAIN sans distinction avec doublons\n",
        "    #- corpusReqName : une liste de tous les nom de PLAIN\n",
        "    #- corpusDicoReqName : numerote chaque PLAIN\n",
        "\n",
        "    print(\"bm25 doc indexing...\")\n",
        "    bm25 = BM25Okapi(corpusDocTokenList) # wtf is that\n",
        "\n",
        "    ndcgCumul=0\n",
        "    corpusReqVec={}\n",
        "    ndcgBM25Cumul=0\n",
        "    nbReq=0\n",
        "    # Calcul du score\n",
        "    from sklearn.metrics import ndcg_score\n",
        "    for req in corpusReqTokenList: #Pour chaque mot dans le plain avec doublons\n",
        "        j=0\n",
        "        reqTokenList = corpusReqTokenList[req]  #liste de mots de PLAIN n°req\n",
        "        doc_scores = bm25.get_scores(reqTokenList) #Computes and returns BM25 scores in relation to every item in corpus.\n",
        "        #print(req)\n",
        "        #print(len(doc_scores)) #nombre de mot dans chaque PLAIN ?\n",
        "        trueDocs = np.zeros(len(corpusDocTokenList)) #Ressort un array de la forme de corpusDocTokenList (1779,)\n",
        "        for docId in corpusDicoDocName: #pour chaque nom de MED\n",
        "            if req in dicReqDocToKeep: #si PLAIN n°req est dans les recommendation\n",
        "                if docId in dicReqDocToKeep[req]: # et si MED n°docId est dans les recommendation de PLAIN\n",
        "                    #get position docId\n",
        "                    posDocId = corpusDicoDocName[docId] #ressort le numéro du MED n°docId\n",
        "                    #print(posDocId)\n",
        "                    #print(req)\n",
        "                    trueDocs[posDocId] = dicReqDocToKeep[req][docId] #arret de la position du numéro de MED est le score déjà donné dans reqdoc ?!\n",
        "                    #print(\"TOKEEP=\",docId)\n",
        "                    #print(trueDocs)\n",
        "        ndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "        print(ndcg_score([trueDocs], [doc_scores],k=ndcgTop))\n",
        "        nbReq = nbReq + 1\n",
        "    ndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "    print(\"ndcg bm25=\",ndcgBM25Cumul)\n",
        "    return ndcgBM25Cumul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_4zY8YbslY0",
        "outputId": "2112b4d7-8530-480c-dcce-dddd5a61de3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndcgTop= 5 nbDocsToKeep= 3192\n",
            "bm25 doc indexing...\n",
            "0.7957607475663049\n",
            "0.5878056198050305\n",
            "0.7734680396957517\n",
            "0.0\n",
            "0.5531464700081437\n",
            "0.3391602052736161\n",
            "0.7957607475663049\n",
            "0.7765732350040717\n",
            "0.9343974612438289\n",
            "0.08479005131840403\n",
            "0.9155230494702449\n",
            "0.5682200679917465\n",
            "0.7123252244714983\n",
            "0.9015629611050541\n",
            "0.5593773280655898\n",
            "0.0\n",
            "0.9532640157442874\n",
            "0.9999999999999999\n",
            "0.9999999999999999\n",
            "0.34519134224686926\n",
            "0.3391602052736161\n",
            "0.9999999999999999\n",
            "0.8687949224876581\n",
            "0.7227265726449517\n",
            "0.69921481985085\n",
            "0.13120507751234178\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.27555194421992346\n",
            "0.0\n",
            "0.2772734273550482\n",
            "0.3937481555797839\n",
            "0.5212164491191417\n",
            "0.24842742676916887\n",
            "0.0\n",
            "0.5065267082131061\n",
            "0.71229144280931\n",
            "0.5307212739772434\n",
            "0.5476854311181707\n",
            "1.0\n",
            "0.69921481985085\n",
            "0.2139862647345275\n",
            "0.6608397947263838\n",
            "0.3391602052736161\n",
            "0.0\n",
            "0.9999999999999999\n",
            "0.9999999999999999\n",
            "0.13120507751234178\n",
            "0.34519134224686926\n",
            "0.46927872602275655\n",
            "0.8687949224876581\n",
            "0.23463936301137828\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "0.36005461457723387\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "0.3391602052736161\n",
            "0.69921481985085\n",
            "0.3391602052736161\n",
            "0.529634717214042\n",
            "0.3391602052736161\n",
            "0.0\n",
            "0.6164336326286642\n",
            "0.3903800499921017\n",
            "0.4852285551163225\n",
            "0.38356636737133554\n",
            "0.8687949224876581\n",
            "0.0\n",
            "0.639945385422766\n",
            "0.0\n",
            "0.7860137352654724\n",
            "0.38356636737133554\n",
            "0.20210734650054754\n",
            "0.5087403079104241\n",
            "0.0\n",
            "0.5\n",
            "0.8687949224876581\n",
            "0.2139862647345275\n",
            "0.0\n",
            "0.2463023887407299\n",
            "0.639945385422766\n",
            "0.639945385422766\n",
            "0.5147714448836773\n",
            "0.3391602052736161\n",
            "0.38356636737133554\n",
            "0.5413996682199069\n",
            "0.0\n",
            "0.0\n",
            "0.9999999999999999\n",
            "ndcg bm25= 0.46335714418774215\n"
          ]
        }
      ],
      "source": [
        "nb_docs = 3192 #all docs\n",
        "#nb_docs = 150 #for tests\n",
        "cumul=run_bm25_only(0,nb_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XJKlUc_WslY1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
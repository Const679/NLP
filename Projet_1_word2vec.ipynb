{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6JToi4oq7SM",
        "outputId": "3447a758-9b44-4c38-e510-2d027dfd018e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.23.5)\n",
            "fatal: destination path 'project1-2023' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25\n",
        "!git clone https://github.com/cr-nlp/project1-2023.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ppa0sUA7q7SQ"
      },
      "outputs": [],
      "source": [
        "import urllib.request as re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from gensim.models import word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT6J14M1q7SR",
        "outputId": "9f2fb892-1884-4c42-d49e-8a9807a4e0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rA1HfLExq7SR"
      },
      "outputs": [],
      "source": [
        "def loadNFCorpus():\n",
        "\tdir = \"./project1-2023/\"\n",
        "\tfilename = dir +\"dev.docs\"\n",
        "\n",
        "\tdicDoc={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\t#print(tabLine)\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\t#print(value)\n",
        "\t\tdicDoc[key] = value\n",
        "\tfilename = dir + \"dev.all.queries\"\n",
        "\tdicReq={}\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.split('\\t')\n",
        "\t\tkey = tabLine[0]\n",
        "\t\tvalue = tabLine[1]\n",
        "\t\tdicReq[key] = value\n",
        "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
        "\tdicReqDoc=defaultdict(dict)\n",
        "\twith open(filename) as file:\n",
        "\t\tlines = file.readlines()\n",
        "\tfor line in lines:\n",
        "\t\ttabLine = line.strip().split('\\t')\n",
        "\t\treq = tabLine[0]\n",
        "\t\tdoc = tabLine[2]\n",
        "\t\tscore = int(tabLine[3])\n",
        "\t\tdicReqDoc[req][doc]=score\n",
        "\n",
        "\treturn dicDoc, dicReq, dicReqDoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QzN-C6A5q7SS"
      },
      "outputs": [],
      "source": [
        "def text2TokenList(text):\n",
        "\tstopword = stopwords.words('english')\n",
        "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
        "\tword_tokens = word_tokenize(text.lower())\n",
        "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
        "\treturn word_tokens_without_stops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Sgu8qB4rq7SS"
      },
      "outputs": [],
      "source": [
        "def modelWord2vec(corpus):\n",
        "    model = word2vec.Word2Vec(sentences=corpus, vector_size=1000, window=5, min_count=1, workers=4)\n",
        "    corpus_embeddings = []\n",
        "    for corp in corpus:\n",
        "        embeddings = [model.wv[word] for word in corp if word in model.wv]\n",
        "        corpus_embeddings.append(sum(embeddings) / len(embeddings))\n",
        "    return corpus_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "74KgPZ4Qq7ST"
      },
      "outputs": [],
      "source": [
        "def run_bm25_only(startDoc,endDoc):\n",
        "\n",
        "    dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
        "    #print(dicReqDoc)\n",
        "\n",
        "    docsToKeep=[]\n",
        "    reqsToKeep=[]\n",
        "    dicReqDocToKeep=defaultdict(dict)\n",
        "\n",
        "    #150\n",
        "    ndcgTop=20\n",
        "    print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",endDoc - startDoc)\n",
        "\n",
        "\n",
        "    #Construction du dictionnaire de Req à garder\n",
        "    i=startDoc\n",
        "    for reqId in dicReqDoc:\n",
        "        if i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
        "            break\n",
        "        for docId in dicReqDoc[reqId]:\n",
        "            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
        "            docsToKeep.append(docId)\n",
        "            i = i + 1\n",
        "        reqsToKeep.append(reqId)\n",
        "    docsToKeep = list(set(docsToKeep))\n",
        "\n",
        "    #print(docsToKeep)\n",
        "\n",
        "    #\"\"\"\n",
        "    allVocab ={}\n",
        "    #Construction corpus des MED\n",
        "    for k in docsToKeep: #Pour chaque MED\n",
        "        docTokenList = text2TokenList(dicDoc[k]) #Transforme les texte de MED en liste\n",
        "        #print(docTokenList)\n",
        "        for word in docTokenList: #Pour chaque mot dans la liste\n",
        "            if word not in allVocab: #S'il n'y est pas déjà\n",
        "                allVocab[word] = word #l'ajoute dans le dictionnaire\n",
        "\n",
        "    allVocabListDoc = list(allVocab)\n",
        "    #print(\"doc vocab=\",allVocabListDoc)\n",
        "\n",
        "\n",
        "    allVocab ={}\n",
        "\n",
        "    #Construction corpus des PLAIN\n",
        "    for k in reqsToKeep: #Pour chaque PLAIN\n",
        "        docTokenList = text2TokenList(dicReq[k]) # Transforme les texte de PLAIN en liste\n",
        "        #print(docTokenList)\n",
        "        for word in docTokenList: # Pour chaque mot dans la liste\n",
        "            if word not in allVocab: #S'il n'y est pas déjà\n",
        "                allVocab[word] = word #l'ajoute dans le dictionnaire\n",
        "    allVocabListReq = list(allVocab)\n",
        "\n",
        "    from rank_bm25 import BM25Okapi\n",
        "\n",
        "    corpusDocTokenList = []\n",
        "    corpusReqTokenList = {}\n",
        "    corpusDocName=[]\n",
        "    corpusDicoDocName={}\n",
        "    i = 0\n",
        "\n",
        "    for k in docsToKeep: #Pour chaque MED\n",
        "        docTokenList = text2TokenList(dicDoc[k]) #Transforme son corpus en liste\n",
        "        corpusDocTokenList.append(docTokenList) #L'ajoute dans la liste de tous les corpus\n",
        "        corpusDocName.append(k) #Ajoute le nom du MED dasn sa propre liste\n",
        "        corpusDicoDocName[k] = i # Attribue un numero a chaque MED\n",
        "        i = i + 1\n",
        "\n",
        "    #print(\"reqs...\")\n",
        "    corpusReqName=[]\n",
        "    corpusDicoReqName={}\n",
        "    i = 0\n",
        "    for k in reqsToKeep: # Pour tous les PLAIN\n",
        "        reqTokenList = text2TokenList(dicReq[k]) #Transforme son corpus en liste\n",
        "        corpusReqTokenList[k] = reqTokenList#créer un dictionnaire des mots pour chaque PLAIN\n",
        "        corpusReqName.append(k) #Ajoute le nom du PLAIN dans sa propre liste\n",
        "        corpusDicoReqName[k] = i# Attribue un numero a chaque PLAIN\n",
        "        i = i + 1\n",
        "\n",
        "    # Arrivé ici, on a :\n",
        "    #- docsToKeep : une liste de tous les nom de MED\n",
        "    #- allVocabListDoc : une liste de tous les mots dans les MED sans distinction sans doublons\n",
        "    #- allVocabListReq : une liste de tous les mots dans les PLAIN sans distinction sans doublons\n",
        "    #- corpusDocTokenList : une liste de tous les mots dans les MED sans distinction avec doublons\n",
        "    #- corpusDocName : same as docsToKeep ?\n",
        "    #- corpusDicoDocName : numerote chaque MED\n",
        "    #- corpusReqTokenList : une liste de tous les mots dans les PLAIN sans distinction avec doublons\n",
        "    #- corpusReqName : une liste de tous les nom de PLAIN\n",
        "    #- corpusDicoReqName : numerote chaque PLAIN\n",
        "\n",
        "    embeddings=modelWord2vec(corpusDocTokenList)\n",
        "\n",
        "    print(\"bm25 doc indexing...\")\n",
        "    bm25 = BM25Okapi(embeddings) # wtf is that\n",
        "\n",
        "    ndcgCumul=0\n",
        "    corpusReqVec={}\n",
        "    ndcgBM25Cumul=0\n",
        "    nbReq=0\n",
        "    # Calcul du score                                                  #rajout de moi\n",
        "    from sklearn.metrics import ndcg_score\n",
        "    for req in corpusReqTokenList: #Pour chaque mot dans le plain avec doublons\n",
        "        j=0\n",
        "        reqTokenList = corpusReqTokenList[req]  #liste de mots de PLAIN n°req\n",
        "        doc_scores = bm25.get_scores(reqTokenList) #Computes and returns BM25 scores in relation to every item in corpus.\n",
        "        #print(req)\n",
        "        #print(len(doc_scores)) #nombre de mot dans chaque PLAIN ?\n",
        "        trueDocs = np.zeros(len(corpusDocTokenList)) #Ressort un array de la forme de corpusDocTokenList (1779,)\n",
        "\n",
        "        for docId in corpusDicoDocName: #pour chaque nom de MED\n",
        "            if req in dicReqDocToKeep: #si PLAIN n°req est dans les recommendation\n",
        "                if docId in dicReqDocToKeep[req]: # et si MED n°docId est dans les recommendation de PLAIN\n",
        "                    #get position docId\n",
        "                    posDocId = corpusDicoDocName[docId] #ressort le numéro du MED n°docId\n",
        "                    #print(posDocId)\n",
        "                    #print(req)\n",
        "                    trueDocs[posDocId] = dicReqDocToKeep[req][docId] #arret de la position du numéro de MED est le score déjà donné dans reqdoc ?!\n",
        "                    #print(\"TOKEEP=\",docId)\n",
        "                    #print(trueDocs)\n",
        "        ndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
        "        nbReq = nbReq + 1\n",
        "    ndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
        "    print(\"ndcg bm25=\",ndcgBM25Cumul)\n",
        "    return ndcgBM25Cumul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUIV-LZTq7SU",
        "outputId": "fea33e77-9464-4b23-f502-74d16ff3d8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ndcgTop= 20 nbDocsToKeep= 3192\n",
            "bm25 doc indexing...\n",
            "ndcg bm25= 0.020026533280516328\n"
          ]
        }
      ],
      "source": [
        "nb_docs = 3192 #all docs\n",
        "#nb_docs = 150 #for tests\n",
        "cumul=run_bm25_only(0,nb_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "csGuVBC6q7SU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m5yEAnBgq7SV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}